{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97a7c94",
   "metadata": {},
   "source": [
    "# Test Different Input-Model Combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b4955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libs\n",
    "import cv2\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from matplotlib.image import imread\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "import ai8x\n",
    "import parse_qat_yaml\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../models/'))\n",
    "\n",
    "# comment out for folded models\n",
    "#b2rgb = importlib.import_module('models.bayer2rgb')\n",
    "\n",
    "# comment out for unfolded model\n",
    "b2rgb = importlib.import_module('models.b2rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943067bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, act_mode_8bit):\n",
    "        self.act_mode_8bit = act_mode_8bit\n",
    "        self.truncate_testset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc25a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mode_8bit = True # For evaluation mode, input/output range: -128, 127\n",
    "\n",
    "test_batch_size = 1\n",
    "\n",
    "args = Args(act_mode_8bit=act_mode_8bit)\n",
    "\n",
    "# change checkpoint according to the model\n",
    "checkpoint_path_b2rgb = '/home/ezgiyucel/repo/ai8x-training/logs/final/checkpoint_q8.pth.tar'\n",
    "\n",
    "qat_yaml_file_used_in_training_b2rgb = '/home/ezgiyucel/repo/ai8x-training/policies/qat_policy.yaml'\n",
    "\n",
    "ai_device = 85\n",
    "round_avg = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675448b",
   "metadata": {},
   "source": [
    "# cats_vs_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeff8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import cats_vs_dogs\n",
    "test_model = importlib.import_module('models.ai85net-cd')\n",
    "data_path = '/data_fast/'\n",
    "checkpoint_path = '/home/ezgiyucel/repo/ai8x-training/logs/cats_vs_dogs_trained/qat_checkpoint_q8.pth.tar'\n",
    "qat_yaml_file_used_in_training = '/home/ezgiyucel/repo/ai8x-training/policies/qat_policy.yaml'\n",
    "_, test_set_inter = cats_vs_dogs.catsdogs_get_datasets((data_path, args), load_train=False, load_test=True, bayer=True, fold=False)\n",
    "_, test_set = cats_vs_dogs.catsdogs_get_datasets((data_path, args), load_train=False, load_test=True, bayer=True)\n",
    "_, test_set_original = cats_vs_dogs.catsdogs_get_datasets((data_path, args), load_train=False, load_test=True, bayer=False)\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37cad7",
   "metadata": {},
   "source": [
    "# camvid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71508d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import camvid\n",
    "test_model = importlib.import_module('models.ai85net-unet')\n",
    "data_path = '/data/raw/CamVid_All/'\n",
    "checkpoint_path = '/home/ezgiyucel/repo/ai8x-synthesis/trained/ai85-camvid-unet-large-q.pth.tar'\n",
    "qat_yaml_file_used_in_training = '/home/ezgiyucel/repo/ai8x-training/policies/qat_policy_camvid.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dadbf1",
   "metadata": {},
   "source": [
    "# cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5676e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import cifar100\n",
    "test_model = importlib.import_module('models.ai85net-nas-cifar')\n",
    "data_path = '/data_fast/'\n",
    "checkpoint_path = '/home/ezgiyucel/repo/ai8x-synthesis/trained/ai85-cifar100-qat8-q.pth.tar'\n",
    "qat_yaml_file_used_in_training = '/home/ezgiyucel/repo/ai8x-training/policies/qat_policy_late_cifar.yaml'\n",
    "_, test_set_inter = cifar100.cifar100_get_datasets((data_path, args), load_train=False, load_test=True, bayer=True, fold=False)\n",
    "_, test_set = cifar100.cifar100_get_datasets((data_path, args), load_train=False, load_test=True, bayer=True)\n",
    "_, test_set_original = cifar100.cifar100_get_datasets((data_path, args), load_train=False, load_test=True, bayer=False)\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336f275",
   "metadata": {},
   "source": [
    "# imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ac0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import imagenet\n",
    "test_model = importlib.import_module('models.ai87net-imagenet-effnetv2')\n",
    "data_path = '/data_ssd/'\n",
    "checkpoint_path = '/home/ezgiyucel/repo/ai8x-synthesis/trained/ai87-imagenet-effnet2-q.pth.tar'\n",
    "qat_yaml_file_used_in_training = '/home/ezgiyucel/repo/ai8x-training/policies/qat_policy_imagenet.yaml'\n",
    "_, test_set_inter = imagenet.imagenet_get_datasets((data_path, args), load_train=False, load_test=True, bayer=True, fold=False)\n",
    "_, test_set = imagenet.imagenet_get_datasets((data_path, args), load_train=False, load_test=True, bayer=True)\n",
    "_, test_set_original = imagenet.imagenet_get_datasets((data_path, args), load_train=False, load_test=True, bayer=False)\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_inter = data.DataLoader(test_set_inter, batch_size=test_batch_size, shuffle=False)\n",
    "test_dataloader = data.DataLoader(test_set, batch_size=test_batch_size, shuffle=False)\n",
    "test_dataloader_original = data.DataLoader(test_set_original, batch_size=test_batch_size, shuffle=False)\n",
    "print(len(test_dataloader))\n",
    "print(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc02416",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "qat_policy_b2rgb = parse_qat_yaml.parse(qat_yaml_file_used_in_training_b2rgb)\n",
    "qat_policy = parse_qat_yaml.parse(qat_yaml_file_used_in_training)\n",
    "\n",
    "ai8x.set_device(device=ai_device, simulate=act_mode_8bit, round_avg=round_avg)\n",
    "\n",
    "# comment out for folded models\n",
    "#model_b2rgb = b2rgb.bayer2rgbnet().to(device)\n",
    "\n",
    "# comment out for unfolded model\n",
    "model_b2rgb = b2rgb.B2RGB().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e4c3a",
   "metadata": {},
   "source": [
    "Run one of the following models according to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model.AI85CatsDogsNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194da0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model.AI85NASCifarNet(bias=\"--use-bias\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47093956",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model.AI85UNetLarge(bias=\"--use-bias\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c308ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model.AI87ImageNetEfficientNetV2(bias=\"--use-bias\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37758e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\n",
    "ai8x.fuse_bn_layers(model_b2rgb)\n",
    "ai8x.fuse_bn_layers(model)\n",
    "\n",
    "# switch model from unquantized to quantized for QAT\n",
    "ai8x.initiate_qat(model_b2rgb, qat_policy_b2rgb)\n",
    "ai8x.initiate_qat(model, qat_policy)\n",
    "\n",
    "checkpoint_b2rgb = torch.load(checkpoint_path_b2rgb,map_location = device)\n",
    "checkpoint = torch.load(checkpoint_path,map_location = device)\n",
    "\n",
    "model_b2rgb.load_state_dict(checkpoint_b2rgb['state_dict'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "ai8x.update_model(model_b2rgb)\n",
    "model_b2rgb = model_b2rgb.to(device)\n",
    "ai8x.update_model(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98a363",
   "metadata": {},
   "source": [
    "# bayer-to-rgb for CamVid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_data(img, fold_ratio=2):\n",
    "    img_folded = None\n",
    "    for i in range(fold_ratio):\n",
    "        for j in range(fold_ratio):\n",
    "            img_subsample = img[i::fold_ratio, j::fold_ratio, :]\n",
    "            if img_folded is not None:\n",
    "                img_folded = np.concatenate((img_folded, img_subsample), axis=2)\n",
    "            else:\n",
    "                img_folded = img_subsample\n",
    "                \n",
    "    return img_folded\n",
    "def Bayer_filter(img):\n",
    "    out =  np.zeros(shape=[img.shape[0],img.shape[1],1],dtype=img.dtype)\n",
    "    for j in range(1,img.shape[1],2):\n",
    "        for i in range(0,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 2]\n",
    "    for j in range(0,img.shape[1],2):\n",
    "        for i in range(0,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 1]\n",
    "    for j in range(1,img.shape[1],2):\n",
    "        for i in range(1,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 1]\n",
    "    for j in range(0,img.shape[1],2):\n",
    "        for i in range(1,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 0]\n",
    "    return out\n",
    "\n",
    "def Bayer_filter_2(img):\n",
    "    out =  np.zeros(shape=[img.shape[0],img.shape[1],3],dtype=img.dtype)\n",
    "    for j in range(1,img.shape[1],2):\n",
    "        for i in range(0,img.shape[0],2):\n",
    "            out[i,j,2] = img[i, j, 2]\n",
    "            out[i,j,1] = 0\n",
    "            out[i,j,0] = 0\n",
    "    for j in range(0,img.shape[1],2):\n",
    "        for i in range(0,img.shape[0],2):\n",
    "            out[i,j,1] = img[i, j, 1]\n",
    "            out[i,j,2] = 0\n",
    "            out[i,j,0] = 0\n",
    "    for j in range(1,img.shape[1],2):\n",
    "        for i in range(1,img.shape[0],2):\n",
    "            out[i,j,1] = img[i, j, 1]\n",
    "            out[i,j,2] = 0\n",
    "            out[i,j,0] = 0\n",
    "    for j in range(0,img.shape[1],2):\n",
    "        for i in range(1,img.shape[0],2):\n",
    "            out[i,j,0] = img[i, j, 0]\n",
    "            out[i,j,1] = 0\n",
    "            out[i,j,2] = 0\n",
    "    return out \n",
    "    \n",
    "folder = \"/data/raw/CamVid_All/CamVid/test orig/\"\n",
    "for filename in os.listdir(folder):\n",
    "    sample_inp = cv2.imread(os.path.join(folder,filename))\n",
    "    sample_inp = cv2.cvtColor(sample_inp, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # comment out for bayer inputs\n",
    "    sample_inp_bayer = Bayer_filter_2(sample_inp)\n",
    "    \n",
    "    # comment out for folded inputs\n",
    "    #sample_inp_bayer = Bayer_filter(sample_inp)\n",
    "    #sample_inp_bayer = fold_data(sample_inp_bayer)\n",
    "    \n",
    "    sample_inp_bayer_folded = (torch.Tensor(sample_inp_bayer).permute([2,0,1]).unsqueeze(0) - 128.).to(device)\n",
    "    model_b2rgb.eval()\n",
    "    with torch.no_grad():\n",
    "        l1_out = model_b2rgb(sample_inp_bayer_folded)\n",
    "        l1_out_np = (l1_out[0].cpu().detach().numpy().transpose(1,2,0)+128)\n",
    "        l1_out_np = l1_out_np.astype(np.uint8)\n",
    "        from PIL import Image\n",
    "        im = Image.fromarray(l1_out_np)\n",
    "        im.save(\"/data/raw/CamVid_All/CamVid/test b2rgb/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29f445",
   "metadata": {},
   "source": [
    "# Bilinear Interpolation for CamVid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74484124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayer_filter(img):\n",
    "    out =  np.zeros(shape=[img.shape[0],img.shape[1],1],dtype=img.dtype)\n",
    "    for j in range(1,img.shape[1],2):\n",
    "        for i in range(0,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 2]\n",
    "    for j in range(0,img.shape[1],2):\n",
    "        for i in range(0,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 1]\n",
    "    for j in range(1,img.shape[1],2):\n",
    "        for i in range(1,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 1]\n",
    "    for j in range(0,img.shape[1],2):\n",
    "        for i in range(1,img.shape[0],2):\n",
    "            out[i,j] = img[i, j, 0]\n",
    "    return out\n",
    "\n",
    "folder = \"/data/raw/CamVid_All/CamVid/test orig/\"\n",
    "for filename in os.listdir(folder):\n",
    "    sample_inp = cv2.imread(os.path.join(folder,filename))\n",
    "    sample_inp = cv2.cvtColor(sample_inp, cv2.COLOR_BGR2RGB)\n",
    "    sample_inp_bayer = Bayer_filter(sample_inp)\n",
    "    img = cv2.cvtColor(sample_inp_bayer,cv2.COLOR_BayerGR2RGB)\n",
    "    from PIL import Image\n",
    "    im = Image.fromarray(img)\n",
    "    im.save(\"/data/raw/CamVid_All/CamVid/test inter/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f325e1",
   "metadata": {},
   "source": [
    "# Bayer-to-RGB + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d513cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b2rgb.eval()\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for image, label in test_dataloader:\n",
    "        image = image.to(device)\n",
    "        \n",
    "        #input_a = (image[0].cpu().detach().numpy().transpose([1, 2, 0])+128).astype(np.uint8)\n",
    "        #plt.figure()\n",
    "        #plt.imshow(input_a)\n",
    "        \n",
    "        primer_out = model_b2rgb(image)\n",
    "        \n",
    "        #result_1 = (primer_out[0].cpu().detach().numpy().transpose([1, 2, 0])+128).astype(np.uint8)\n",
    "        #plt.figure()\n",
    "        #plt.imshow(result_1)\n",
    "        \n",
    "        model_out = model(primer_out)\n",
    "        result = np.argmax(model_out.cpu())\n",
    "        if(label == result):\n",
    "            correct = correct + 1 \n",
    "print(\"accuracy:\")\n",
    "print(correct / len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc140e4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for image, label in test_dataloader_original:\n",
    "        image = image.to(device)\n",
    "        model_out = model(image)\n",
    "        result = np.argmax(model_out.cpu())\n",
    "        if(label == result):\n",
    "            correct = correct + 1\n",
    "print(\"accuracy:\")\n",
    "print(correct / len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea82bf8",
   "metadata": {},
   "source": [
    "# Bilinear Interpolation + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for image, label in test_dataloader_inter:\n",
    "        image = image.to(device)\n",
    "        img = (128+(image[0].cpu().detach().numpy().transpose(1,2,0))).astype(np.uint8)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BayerGR2RGB)\n",
    "        out_tensor = torch.Tensor(((img.transpose(2,0,1).astype(np.float32))/128-1)).to(device)\n",
    "        out_tensor = out_tensor.unsqueeze(0)\n",
    "        model_out = model(out_tensor)\n",
    "        result = np.argmax(model_out.cpu())\n",
    "        if(label == result):\n",
    "            correct = correct + 1\n",
    "print(\"accuracy:\")\n",
    "print(correct / len(test_set))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
